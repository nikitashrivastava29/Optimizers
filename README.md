# Optimizers
This repository contains the codes for all papers studied in the the survey paper "Momentum Based Methods for Distributed and Federated Learning"

Following are the papers that have been included in the repository till now:
1. Asynchrony begets Momentum, with an application to Deep Learning
2. SLOWMO: Improving Communication-Efficient Distributed SGD with Slow Momentum
3. CADA: Communication-Adaptive Distributed Adam
4. Taming Momentum in a Distributed Asynchronous Environment
5. Quasi-Global Momentum: Accelerating Decentralized Deep Learning on Heterogeneous Data

| S.No. | Paper | Method | Link | Source |
| --- | --- | --- | --- | --- |
| 1. | Asynchrony begets Momentum, with an application to Deep Learning | NaN | https://arxiv.org/abs/1605.09774 | NaN |
| 2. | SLOWMO: Improving Communication-Efficient Distributed SGD with Slow Momentum | SlowMo | https://arxiv.org/abs/1910.00643 | https://github.com/facebookresearch/fairscale/blob/main/fairscale/experimental/nn/data_parallel/gossip/distributed.py |
| 3. | CADA: Communication-Adaptive Distributed Adam | CADA | https://arxiv.org/abs/2012.15469 | https://github.com/ChrisYZZ/CADA-master/blob/main/Python/MNIST%20code/main_CADA_MNIST.py | 
| 4. | Taming Momentum in a Distributed Asynchronous Environment | DANA | https://arxiv.org/abs/1907.11612 | NaN |
| 5. |  Quasi-Global Momentum: Accelerating Decentralized Deep Learning on Heterogeneous Data | Quasi-Global Momentum | https://arxiv.org/abs/2102.04761 | https://github.com/epfml/quasi-global-momentum/tree/master?tab=readme-ov-file | 

