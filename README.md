# Optimizers
This repository contains the codes for all papers studied in the the survey paper "Momentum Based Methods for Distributed and Federated Learning"

Following are the papers that have been included in the repository till now:
1. Asynchrony begets Momentum, with an application to Deep Learning
2. SLOWMO: Improving Communication-Efficient Distributed SGD with Slow Momentum
3. CADA: Communication-Adaptive Distributed Adam
4. Taming Momentum in a Distributed Asynchronous Environment
5. Stochastic Gradient Push for Distributed Deep Learning

| S.No. | Paper | Method | Link | Source |
| --- | --- | --- | --- | --- |
| 1. | Asynchrony begets Momentum, with an application to Deep Learning | DANA | https://arxiv.org/abs/1605.09774 | NaN |
| 2. | SLOWMO: Improving Communication-Efficient Distributed SGD with Slow Momentum | SlowMo | https://arxiv.org/abs/1910.00643 | https://github.com/facebookresearch/fairscale/blob/main/fairscale/experimental/nn/data_parallel/gossip/distributed.py |
| 3. | CADA: Communication-Adaptive Distributed Adam | CADA | https://arxiv.org/abs/2012.15469 | https://github.com/ChrisYZZ/CADA-master/blob/main/Python/MNIST%20code/main_CADA_MNIST.py | 
| 4. | Taming Momentum in a Distributed Asynchronous Environment | NaN | https://arxiv.org/abs/1907.11612 | NaN |
| 5. | Stochastic Gradient Push for Distributed Deep Learning | Stochastic Gradient Push | https://arxiv.org/abs/1811.10792 | https://github.com/facebookresearch/stochastic_gradient_push?tab=readme-ov-file | 

